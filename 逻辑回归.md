
## 分类
在分类问题中，我们尝试预测的是结果是否属于某一个类（例如正确或错误）。

如果我们要用线性回归算法来解决一个分类问题，对于分类，  取值为 0 或者1，但如果你使用的是线性回归，那么假设函数的输出值可能远大于 1，或者远小于0，即使所有训练样本的标签  都等于 0 或 1。尽管我们知道标签应该取值0 或者1，但是如果算法得到的值远大于1或者远小于0的话，就会感觉很奇怪。所以我们在接下来的要研究的算法就叫做逻辑回归算法，这个算法的性质是：它的输出值永远在0到 1 之间。

## 假说表示
分类问题中，要用什么样的函数来表示我们的假设？前我们说过，希望我们的分类器的输出值在0和1之间，因此，我们希望想出一个满足某个性质的假设函数，这个性质是它的预测值要在0和1之间。

线性回归模型，因为其预测的值可以超越[0,1]的范围，并不适合解决这样的问题。

逻辑回归区别于线性回归是一个分类函数， 就是说逻辑回归我们更关心的是目标函数的值（分布在0,1之间），表示是正类的概率。

那么如何找到一个假设函数去表达这个函数呢？根据之前的做法，假设一个数据集有n个特征，然后我在每个特征前乘以一个系数 最后把这些加起来 计算结果，即对所有的特征作加权处理。这样得到的结果是在正负无穷的范围 如何迎合我们要表达的上面的在0和1之前的概率函数呢？

这个时候Sigmoid函数解决了我们的问题，它的输入为z，我们可以把之前的加权处理的结果加上这个函数的限制 可以使得结果在0到1之前 这样我们就得到了我们的想要的假设函数：

![](https://raw.githubusercontent.com/mortimer-cra/mypic/master/20190906111156.png)

## 通过决策边界来理解理解逻辑回归的假设函数在计算什么？
![](https://raw.githubusercontent.com/mortimer-cra/mypic/master/20190906113325.png)
也就是说可以根据参数x轴的θ^T x的计算结果是什么可以知道对应的的值 从而预测y的值。
下面看下具体的模型用假设参数表达出来后 对应的预测值得举例：

第一个：
![](https://raw.githubusercontent.com/mortimer-cra/mypic/master/20190906113856.png)
并且参数θ 是向量[-3 1 1]。 ==则当-3+x_1+x_2≥0==，即x_1+x_2≥3时，套在s型函数后 ==模型将预测 y=1==。 我们可以绘制直线x_1+x_2=3，这条线便是我们模型的==分界线==，将预测为1的区域和预测为 0的区域分隔开。
![](https://raw.githubusercontent.com/mortimer-cra/mypic/master/20190906114353.png)

---

再来一个：
![](https://raw.githubusercontent.com/mortimer-cra/mypic/master/20190906115409.png)
我们可以用非常复杂的模型来适应非常复杂形状的判定边界。

## 为拟合逻辑回归模型的参数θ定义代价函数 
![](https://raw.githubusercontent.com/mortimer-cra/mypic/master/20190906140411.png)
这意味着我们的代价函数有许多局部最小值，这将影响梯度下降算法寻找全局最小值。

线性回归的损失函数是平方损失。逻辑回归的损失函数是对数损失函数（直接给结论），定义如下：
![](https://raw.githubusercontent.com/mortimer-cra/mypic/master/20190906143141.png)
因为h_θ (x)的范围是0-1的 所以取log函数的0-1区间值，h_θ (x)是预测值， y是真实值 这两个都是只有两种结果 即 0 或者 1。 如果假设真实值y=1 那么h_θ (x)也=1的时候 cost损失为0，反之cost为无穷大。另一种情况类似，如下图所示：
![](https://raw.githubusercontent.com/mortimer-cra/mypic/master/20190906144002.png)

因为y永远是等于1或者0的 最终简化后 代价函数为：
![](https://raw.githubusercontent.com/mortimer-cra/mypic/master/20190906144114.png)

## 利用梯度下降算法求解代价函数的最小值找到最佳的参数θ
由上代价函数可知 逻辑回归的代价函数为：
![](https://raw.githubusercontent.com/mortimer-cra/mypic/master/20190906145211.png)

在得到这样一个代价函数以后，我们便可以用梯度下降算法来求得能使代价函数最小的参数了。算法为：

![](https://raw.githubusercontent.com/mortimer-cra/mypic/master/20190906145727.png)

结果看上去和线性回归一样 但是线性回归h_θ (x)是θ转置*x 但是逻辑回归是s函数。

找到合适的参数θ 令结果最小。找到最佳的θ后，那么就可以利用下面的函数进行预测新的x对应的y值了，大于0.5那么就是正分类 反之就是负分类
![](https://raw.githubusercontent.com/mortimer-cra/mypic/master/20190906145314.png)

## 除了梯度下降求解逻辑回归代价函数的优化算法
记住下面两个就好
![](https://raw.githubusercontent.com/mortimer-cra/mypic/master/20190906150241.png)

## 多分类问题：一对多
- 邮件标签：工作、朋友、家人、爱好
- 病情：没有病、冷感冒、流行感冒
- 天气：晴天、多云、雨、雪

![](https://raw.githubusercontent.com/mortimer-cra/mypic/master/20190906151539.png)
> 其实就是把一个三分类的问题 拆分为3个二分类的问题 为每一个类训练一个训练器出来 然后来了一个新的值 全带进去算出来 哪个最大就是属于那个分类器的值。
