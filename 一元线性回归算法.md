# 模型表达

记号：

- m: 表示训练样本的个数
- x: 表述输入(input)变量/特征
- y: 表述输出(output)变量/目标(target)变量

![](https://raw.githubusercontent.com/mortimer-cra/mypic/master/20190819155052.png)

![](https://raw.githubusercontent.com/mortimer-cra/mypic/master/20190819155333.png)

# 代价函数(cost function)

![](https://raw.githubusercontent.com/mortimer-cra/mypic/master/20190819155711.png)
![](https://raw.githubusercontent.com/mortimer-cra/mypic/master/20190819155720.png)
![](https://raw.githubusercontent.com/mortimer-cra/mypic/master/20190819155737.png)


## 损失函数(Loss function)
> 是定义在单个训练样本上的，也就是就算一个样本的误差，比如我们想
> 要分类，就是预测的类别和实际类别的区别，是一个样本的哦，用L表示

![](https://raw.githubusercontent.com/mortimer-cra/mypic/master/20190819162218.png)
## 代价函数(Cost function)
> 是定义在整个训练集上面的，也就是所有样本的误差的总和的平均，
> 也就是损失函数的总和的平均，有没有这个平均其实不会影响最后的参数的求解结果。

![](https://raw.githubusercontent.com/mortimer-cra/mypic/master/20190819162234.png)

# 代价函数感官认识一
![](https://raw.githubusercontent.com/mortimer-cra/mypic/master/20190819162953.png)

![](https://raw.githubusercontent.com/mortimer-cra/mypic/master/20190819163230.png)

![](https://raw.githubusercontent.com/mortimer-cra/mypic/master/20190819163344.png)
![](https://raw.githubusercontent.com/mortimer-cra/mypic/master/20190819164139.png)

# 代价函数感官认识二
![](https://raw.githubusercontent.com/mortimer-cra/mypic/master/20190819164356.png)
![](https://raw.githubusercontent.com/mortimer-cra/mypic/master/20190819170146.png)

不是最小的：

---
![](https://raw.githubusercontent.com/mortimer-cra/mypic/master/20190819170509.png)

![](https://raw.githubusercontent.com/mortimer-cra/mypic/master/20190819170549.png)

最小的情况：

---

![](https://raw.githubusercontent.com/mortimer-cra/mypic/master/20190819170614.png)


# Gradient descent（梯度下降算法）
![](https://raw.githubusercontent.com/mortimer-cra/mypic/master/20190819171427.png)

图像直观表达：

![](https://raw.githubusercontent.com/mortimer-cra/mypic/master/20190819171450.png)

用数学表达：
![](https://raw.githubusercontent.com/mortimer-cra/mypic/master/20190819171523.png)
> 就是每走一步就要改变θ0以及θ1的值 α表示的是我一步的步长的多少 后面θj的偏导 指的是下山最快的方向


# Gradient descent（梯度下降算法）直观感受

> 用来理解后面的偏导项的含义

![](https://raw.githubusercontent.com/mortimer-cra/mypic/master/20190820134533.png)
进行求导：
![](https://raw.githubusercontent.com/mortimer-cra/mypic/master/20190820134713.png)

a的取值：

---

![](https://raw.githubusercontent.com/mortimer-cra/mypic/master/20190820135014.png)

![](https://raw.githubusercontent.com/mortimer-cra/mypic/master/20190820135226.png)
![](https://raw.githubusercontent.com/mortimer-cra/mypic/master/20190820135421.png)
![](https://raw.githubusercontent.com/mortimer-cra/mypic/master/20190820135442.png)

# 梯度下降算法求解线性回归模型
![](https://raw.githubusercontent.com/mortimer-cra/mypic/master/20190820135511.png)

分别当j=0 和j=1时候 对θj求偏导 即求θ1和θ0各自的偏导数：


![](https://raw.githubusercontent.com/mortimer-cra/mypic/master/20190820140337.png)

![](https://raw.githubusercontent.com/mortimer-cra/mypic/master/20190820140355.png)

![](https://raw.githubusercontent.com/mortimer-cra/mypic/master/20190820140409.png)

初始假设
![](https://raw.githubusercontent.com/mortimer-cra/mypic/master/20190820140437.png)
最终：
![](https://raw.githubusercontent.com/mortimer-cra/mypic/master/20190820140505.png)

# 理解与总结
有一个数据集 为了拟合属于这些数据集的线性表示 需要拟合一个线性的函数表达 使得所有的数据点距离这根线的距离最小 也就是损失最小 这里采用了一个求方差的方式 把每个点距离直线的距离进行平方 然后加起来得到一个值 这个值最小的时候那么这个函数就最合适了。 这个就是代价表达式 这个表达式的前面乘什么东西都无所谓 因为目的是使得总和最小 你除以个数 只是为了更直观地看到单个值的情况 1/2可以理解为半个单个值 其实就是为了约掉求导后的那个二次方让方程看起来规整一些 这些无所谓的。 现在得到这个函数后我想知道怎么求得最小值 如果一个一个带进去去试 那是不可行的。这里就引出了 一个梯度下降算法。

梯度下降算法很简单，前面的方程用一个三维空间表示出来就是上面那个图 我现在要求的是整个图的最低点 或者说局部最低点 相对应的那个维度x和y的值 要求这些值。梯度下降法的思想就是先在这个图上随便找个点 这个点在哪不重要，然后求这个点在这个图的两个维度的偏导 这个偏导的方向就是下降最快的方向 长度就是梯度的长度 这个梯度的长度先不去理解他了 为了更快的到达终点 可以搞个超参数乘以这个长度 然后再以最初找到的点去减去这个值 然后就下降了一节 随之更新这个初始值 不断的循环找到最低点 也就是找到了损失最小的点 可以得到相应的两个或者多个参数的解  这么一个算法 简单的一批

更深刻的直观了解看这：
https://www.matongxue.com/madocs/222.html